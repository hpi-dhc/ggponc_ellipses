{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Disable weights and biases (if installed)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "#os.environ[\"WANDB_PROJECT\"] = \"ggponc_ellipses\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Florian.Borchert/miniconda3/envs/ellipses/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from generative.dataset import EllipsesDataset\n",
    "from generative.run_experiment import get_training_args, get_trainer, get_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from hydra import compose, initialize\n",
    "\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(config_path=Path('..'), job_name='foo', version_base='1.1')\n",
    "config = compose(config_name='experiment.yaml')\n",
    "config.model_name = \"google/mt5-base\"\n",
    "config.metrics = ['exact_match', 'google_bleu']\n",
    "config.learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/home/Florian.Borchert/miniconda3/envs/ellipses/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = get_training_args(config, report_to=None)\n",
    "tokenizer = get_tokenizer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_ellipses = pd.read_csv('../../ggponc_annotation/notebooks/ggponc_ccnfs.tsv', sep='\\t')\n",
    "df_controls = pd.read_csv('../../ggponc_annotation/notebooks/ggponc_cnfs_controls_small.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2241, 462, 462, 2269, 447, 449)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cnfs = df_ellipses[df_ellipses.split == 'train']\n",
    "valid_cnfs = df_ellipses[df_ellipses.split == 'dev']\n",
    "test_cnfs = df_ellipses[df_ellipses.split == 'test']\n",
    "\n",
    "train_controls = df_controls[df_controls.split == 'train']\n",
    "valid_controls = df_controls[df_controls.split == 'dev']\n",
    "test_controls = df_controls[df_controls.split == 'test']\n",
    "\n",
    "len(train_cnfs), len(valid_cnfs),  len(test_cnfs), len(train_controls), len(valid_controls), len(test_controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = EllipsesDataset(pd.concat([train_cnfs.raw_sentence]), pd.concat([train_cnfs.full_resolution]), tokenizer)\n",
    "val_data = EllipsesDataset(pd.concat([valid_cnfs.raw_sentence]), pd.concat([valid_cnfs.full_resolution]), tokenizer)\n",
    "test_data = EllipsesDataset(pd.concat([test_cnfs.raw_sentence]), pd.concat([test_cnfs.full_resolution]), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = EllipsesDataset(pd.concat([train_cnfs.raw_sentence, train_controls.raw_sentence]), pd.concat([train_cnfs.full_resolution, train_controls.raw_sentence]), tokenizer)\n",
    "#val_data = EllipsesDataset(pd.concat([valid_cnfs.raw_sentence, valid_controls.raw_sentence]), pd.concat([valid_cnfs.full_resolution, valid_controls.raw_sentence]), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.num_train_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/Florian.Borchert/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/Florian.Borchert/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/Florian.Borchert/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "trainer = get_trainer(config, tokenizer, training_args, train_data, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Florian.Borchert/miniconda3/envs/ellipses/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2241\n",
      "  Num Epochs = 30\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 8430\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8430' max='8430' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8430/8430 1:20:05, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact Match</th>\n",
       "      <th>Google Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.607400</td>\n",
       "      <td>0.650882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.313107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.517200</td>\n",
       "      <td>0.224983</td>\n",
       "      <td>0.194805</td>\n",
       "      <td>0.700083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.225600</td>\n",
       "      <td>0.082987</td>\n",
       "      <td>0.569264</td>\n",
       "      <td>0.794333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.138800</td>\n",
       "      <td>0.074682</td>\n",
       "      <td>0.638528</td>\n",
       "      <td>0.864628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.129300</td>\n",
       "      <td>0.067145</td>\n",
       "      <td>0.638528</td>\n",
       "      <td>0.881820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.089500</td>\n",
       "      <td>0.062640</td>\n",
       "      <td>0.675325</td>\n",
       "      <td>0.907652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.094900</td>\n",
       "      <td>0.052798</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.935579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.072400</td>\n",
       "      <td>0.043203</td>\n",
       "      <td>0.774892</td>\n",
       "      <td>0.931939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.044705</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.945208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.057200</td>\n",
       "      <td>0.041364</td>\n",
       "      <td>0.777056</td>\n",
       "      <td>0.940287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.049200</td>\n",
       "      <td>0.040044</td>\n",
       "      <td>0.790043</td>\n",
       "      <td>0.939245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.041900</td>\n",
       "      <td>0.040498</td>\n",
       "      <td>0.794372</td>\n",
       "      <td>0.954195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.034200</td>\n",
       "      <td>0.040867</td>\n",
       "      <td>0.796537</td>\n",
       "      <td>0.954624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.033400</td>\n",
       "      <td>0.040792</td>\n",
       "      <td>0.794372</td>\n",
       "      <td>0.961728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.039819</td>\n",
       "      <td>0.807359</td>\n",
       "      <td>0.974887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.025200</td>\n",
       "      <td>0.040748</td>\n",
       "      <td>0.811688</td>\n",
       "      <td>0.963877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.039203</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.977293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>0.041549</td>\n",
       "      <td>0.824675</td>\n",
       "      <td>0.975157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.043204</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.974638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>0.040182</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.975203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.028500</td>\n",
       "      <td>0.040788</td>\n",
       "      <td>0.824675</td>\n",
       "      <td>0.974956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>0.041207</td>\n",
       "      <td>0.822511</td>\n",
       "      <td>0.979455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.041164</td>\n",
       "      <td>0.831169</td>\n",
       "      <td>0.976700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>0.040565</td>\n",
       "      <td>0.824675</td>\n",
       "      <td>0.974320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.041504</td>\n",
       "      <td>0.835498</td>\n",
       "      <td>0.978974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.042139</td>\n",
       "      <td>0.829004</td>\n",
       "      <td>0.979011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.041996</td>\n",
       "      <td>0.829004</td>\n",
       "      <td>0.979702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.042316</td>\n",
       "      <td>0.826840</td>\n",
       "      <td>0.978624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.041793</td>\n",
       "      <td>0.829004</td>\n",
       "      <td>0.979488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.041767</td>\n",
       "      <td>0.829004</td>\n",
       "      <td>0.979617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8430, training_loss=0.3067725738950189, metrics={'train_runtime': 4805.7988, 'train_samples_per_second': 13.989, 'train_steps_per_second': 1.754, 'total_flos': 1.58202661833216e+16, 'train_loss': 0.3067725738950189, 'epoch': 30.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 462\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='58' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8/58 00:08 < 01:01, 0.81 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m eval_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(eval_metrics)\n\u001b[1;32m      4\u001b[0m test_metrics \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(test_data, metric_key_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ellipses/lib/python3.10/site-packages/transformers/trainer_seq2seq.py:78\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m gen_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_beams\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     gen_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_beams\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m gen_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_beams\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgeneration_num_beams\n\u001b[1;32m     75\u001b[0m )\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ellipses/lib/python3.10/site-packages/transformers/trainer.py:2774\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2771\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   2773\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 2774\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2775\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2777\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   2778\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   2779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2782\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2784\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   2785\u001b[0m output\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m   2786\u001b[0m     speed_metrics(\n\u001b[1;32m   2787\u001b[0m         metric_key_prefix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2791\u001b[0m     )\n\u001b[1;32m   2792\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ellipses/lib/python3.10/site-packages/transformers/trainer.py:2952\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2949\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   2951\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 2952\u001b[0m loss, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2953\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n",
      "File \u001b[0;32m~/miniconda3/envs/ellipses/lib/python3.10/site-packages/transformers/trainer_seq2seq.py:198\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     generation_inputs \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmain_input_name]\n\u001b[0;32m--> 198\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# in case the batch is shorter than max length, the output should be padded\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gen_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m generated_tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m gen_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/ellipses/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ellipses/lib/python3.10/site-packages/transformers/generation_utils.py:1390\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1386\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_return_sequences has to be 1, but is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_return_sequences\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m when doing greedy search.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1387\u001b[0m         )\n\u001b[1;32m   1389\u001b[0m     \u001b[38;5;66;03m# 10. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1391\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1395\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1396\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1398\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1399\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1400\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_sample_gen_mode:\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;66;03m# 10. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1404\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(\n\u001b[1;32m   1405\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m   1406\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1410\u001b[0m         renormalize_logits\u001b[38;5;241m=\u001b[39mrenormalize_logits,\n\u001b[1;32m   1411\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/ellipses/lib/python3.10/site-packages/transformers/generation_utils.py:1838\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1835\u001b[0m     unfinished_sequences \u001b[38;5;241m=\u001b[39m unfinished_sequences\u001b[38;5;241m.\u001b[39mmul((next_tokens \u001b[38;5;241m!=\u001b[39m eos_token_id)\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m   1837\u001b[0m \u001b[38;5;66;03m# stop when each sentence is finished, or if we exceed the maximum length\u001b[39;00m\n\u001b[0;32m-> 1838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unfinished_sequences\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mstopping_criteria\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1839\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m synced_gpus:\n\u001b[1;32m   1840\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ellipses/lib/python3.10/site-packages/transformers/generation_stopping_criteria.py:111\u001b[0m, in \u001b[0;36mStoppingCriteriaList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mStoppingCriteriaList\u001b[39;00m(\u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;129m@add_start_docstrings\u001b[39m(STOPPING_CRITERIA_INPUTS_DOCSTRING)\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor, scores: torch\u001b[38;5;241m.\u001b[39mFloatTensor, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28many\u001b[39m(criteria(input_ids, scores) \u001b[38;5;28;01mfor\u001b[39;00m criteria \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax_length\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mint\u001b[39m]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_metrics = trainer.evaluate(val_data)\n",
    "print(eval_metrics)\n",
    "\n",
    "test_metrics = trainer.evaluate(test_data, metric_key_prefix='test')\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Text2TextGenerationPipeline\n",
    "from evaluation import error_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Text2TextGenerationPipeline(model=trainer.model, tokenizer=tokenizer, max_length=500, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hauptrisikofaktoren für das Auftreten eines Mundhöhlenkarzinoms sind chronischer Tabak- oder Alkoholabusus, wesentlich seltener auch andere Faktoren.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_cnfs.raw_sentence.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hauptrisikofaktoren für das Auftreten eines Mundhöhlenkarzinoms sind chronischer Tabakabusus oder Alkoholabusus, wesentlich seltener auch andere Faktoren.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_cnfs.full_resolution.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import error_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_analysis(predictions, gt_resolutions, original_sentences):\n",
    "    d = difflib.Differ()\n",
    "    res = []\n",
    "\n",
    "    for pred_gen, true, sent in zip(predictions, gt_resolutions, original_sentences):\n",
    "        entry = {'pred' : pred_gen, 'ground_truth' : true, 'original' : sent}    \n",
    "        if pred_gen == true:\n",
    "            entry['error_type'] = 'tp'\n",
    "        elif pred_gen == sent:\n",
    "            entry['error_type'] = 'fn'\n",
    "        else:\n",
    "            op_codes = difflib.SequenceMatcher(None, true, pred_gen).get_opcodes()\n",
    "            counts = Counter([o[0] for o in op_codes])\n",
    "            del counts[\"equal\"]\n",
    "            if len(counts) > 1:\n",
    "                entry['error_type'] = 'complex'\n",
    "            else:\n",
    "                entry['error_type'] = list(counts.keys())[0]\n",
    "        res.append(entry)\n",
    "    return pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_errors(out, sample):\n",
    "    gen_text = [o['generated_text'] for o in out]\n",
    "    errors = error_analysis(gen_text, sample.full_resolution, sample.raw_sentence)\n",
    "    display(pd.concat([errors.error_type.value_counts(), errors.error_type.value_counts() / len(errors)], axis=1))    \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 14s, sys: 98.8 ms, total: 4min 14s\n",
      "Wall time: 4min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "out_valid = pipeline(list(valid_cnfs.raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>error_type</th>\n",
       "      <th>error_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tp</th>\n",
       "      <td>382</td>\n",
       "      <td>0.826840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delete</th>\n",
       "      <td>27</td>\n",
       "      <td>0.058442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>replace</th>\n",
       "      <td>18</td>\n",
       "      <td>0.038961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insert</th>\n",
       "      <td>16</td>\n",
       "      <td>0.034632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fn</th>\n",
       "      <td>10</td>\n",
       "      <td>0.021645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>complex</th>\n",
       "      <td>9</td>\n",
       "      <td>0.019481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         error_type  error_type\n",
       "tp              382    0.826840\n",
       "delete           27    0.058442\n",
       "replace          18    0.038961\n",
       "insert           16    0.034632\n",
       "fn               10    0.021645\n",
       "complex           9    0.019481"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "errors_valid = calculate_errors(out_valid, valid_cnfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed = nltk.edit_distance\n",
    "def metric(p, g, o):\n",
    "    d = ed(p,g)\n",
    "    k = ed(p,o)\n",
    "    l = ed(o,g)\n",
    "    if d == 0:\n",
    "        return 1\n",
    "    return 1 - (d / (k + l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9313555280996356"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_score = errors_valid.apply(lambda r: metric(r['pred'], r['ground_truth'], r['original']), axis=1)\n",
    "my_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 42s, sys: 89.3 ms, total: 4min 42s\n",
      "Wall time: 4min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "out_test = pipeline(list(test_cnfs.raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>error_type</th>\n",
       "      <th>error_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tp</th>\n",
       "      <td>376</td>\n",
       "      <td>0.813853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delete</th>\n",
       "      <td>26</td>\n",
       "      <td>0.056277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>complex</th>\n",
       "      <td>20</td>\n",
       "      <td>0.043290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insert</th>\n",
       "      <td>16</td>\n",
       "      <td>0.034632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>replace</th>\n",
       "      <td>13</td>\n",
       "      <td>0.028139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fn</th>\n",
       "      <td>11</td>\n",
       "      <td>0.023810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         error_type  error_type\n",
       "tp              376    0.813853\n",
       "delete           26    0.056277\n",
       "complex          20    0.043290\n",
       "insert           16    0.034632\n",
       "replace          13    0.028139\n",
       "fn               11    0.023810"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "errors_test = calculate_errors(out_test, test_cnfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9269631059037321"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_score = errors_test.apply(lambda r: metric(r['pred'], r['ground_truth'], r['original']), axis=1)\n",
    "my_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ellipses]",
   "language": "python",
   "name": "conda-env-ellipses-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f7616dd95153615ba76d82383ee4b763d06514f4c395e85d9efff1c9a575639"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForSeq2SeqLM,Text2TextGenerationPipeline\n",
    "from evaluation import error_analysis, get_scores\n",
    "from dataset import load_data, get_dataloader\n",
    "from generative.transformers_util import get_training_args, get_trainer, get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import hydra\n",
    "from hydra import compose, initialize\n",
    "\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(config_path=Path('..'), job_name='foo', version_base='1.1')\n",
    "config = compose(config_name='experiment.yaml')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "openai.api_key = config.api_key"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_args = get_training_args(config, report_to=\"none\")\n",
    "tokenizer = get_tokenizer(config)\n",
    "\n",
    "base_path = Path('..')\n",
    "train_df, val_df, test_df = load_data(base_path / config.data.cnf_tsv_path, base_path / config.data.controls_tsv_path)\n",
    "train_dataset, val_dataset, test_dataset = get_dataloader(train_df, val_df, test_df, tokenizer)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"../data/model\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation of taking the top 1 result from our LLM and comparing it to the resolution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pipeline = Text2TextGenerationPipeline(model=model, tokenizer=tokenizer, max_length=config.generation_max_length, device=0)\n",
    "\n",
    "resolutions = list(val_df.full_resolution)\n",
    "samples = list(val_df.raw_sentence)\n",
    "predictions = pipeline(samples)\n",
    "\n",
    "errors = error_analysis([prediction[\"generated_text\"] for prediction in predictions], resolutions, samples)\n",
    "get_scores(errors, \"eval\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluating the improvement produced by taking the top k generations and comparing all of them to the resolution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_top_k(model, tokenizer, data, beams, generation_max_length=config.generation_max_length):\n",
    "    pipeline = Text2TextGenerationPipeline(model=model, tokenizer=tokenizer, max_length=generation_max_length, num_beams=beams, num_return_sequences=beams, device=0)\n",
    "\n",
    "    originals = list(data.raw_sentence)\n",
    "    resolutions = list(data.full_resolution)\n",
    "    outputs = pipeline(originals)\n",
    "\n",
    "    predictions = []\n",
    "    for i, resolution in enumerate(resolutions):\n",
    "        generations = [entry['generated_text'] for entry in outputs[i]]\n",
    "        scores = [relative_edit_distance(gen, resolution, originals[i]) for gen in generations]\n",
    "        if max(scores) == 1 and scores.index(max(scores)) != 0:\n",
    "            print(generations)\n",
    "            print(scores.index(max(scores)))\n",
    "            print(resolution)\n",
    "        predictions.append(generations[scores.index(max(scores))])\n",
    "\n",
    "    errors = error_analysis(predictions, resolutions, list(val_df.raw_sentence))\n",
    "    scores = get_scores(errors, \"eval\")\n",
    "\n",
    "    return scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k = 5\n",
    "evaluate_top_k(model, tokenizer, val_df, k)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using ChatGPT/GPT to determine the best fit of the top k options"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_prompt(original, predictions):\n",
    "    beginning = \"Ich werde dir im Folgenden einen Satz zeigen, welcher sogennannte Koordinationsellipsen enthält. Das Ziel ist es diese zu aufzulösen. Ein Beispiel wäre 'Ibrutinib, ein Inhibitor der Bruton-Tyrosinkinase (BTK), ist in Deutschland als Erstlinien- und Rezidivtherapiee in der CLL zugelassen.' Die richtige Auflösung wäre 'Ibrutinib, ein Inhibitor der Bruton-Tyrosinkinase (BTK), ist in Deutschland als Erstlinientherapie und Rezidivtherapiee in der CLL zugelassen.' Ich werde dir für meine Beispiele Antwortmöglichkeiten geben und du sollst dann entscheiden, welche dieser Optionen die Koordinationsellipsen korrekt auflöst.\\n\\n\"\n",
    "    original = f\"Mein Satz: '{original}'\\n\\n\"\n",
    "    answers = \"Deine Antwortmöglichkeiten:\\n\" + \"\".join(f\"{i+1}) '{prediction}'\\n\" for i, prediction in enumerate(predictions))\n",
    "    end = \"\\nWelche Antwort ist die richtige? Antworte nur mit der Zahl und keiner Erklärung\"\n",
    "\n",
    "    return beginning + original + answers + end"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_openai_response_chatgpt(prompt):\n",
    "    return openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-0301\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=100,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0\n",
    "    )[\"choices\"][0][\"message\"][\"content\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_openai_response_gpt3(prompt):\n",
    "    return openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt= prompt,\n",
    "        temperature=0,\n",
    "        max_tokens=100,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0\n",
    "    )[\"choices\"][0][\"text\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import Text2TextGenerationPipeline\n",
    "from evaluation import relative_edit_distance, error_analysis, get_scores\n",
    "import re\n",
    "\n",
    "def generate_best_fit(samples, resolutions, outputs):\n",
    "    predictions = []\n",
    "    for i, resolution in enumerate(resolutions):\n",
    "        generations = [entry['generated_text'] for entry in outputs[i]]\n",
    "        answer = get_openai_response_chatgpt(generate_prompt(samples[i], generations))\n",
    "\n",
    "        numbers = re.findall(r'\\d+', answer)\n",
    "        if len(numbers) > 1:\n",
    "            print(f'more numbers than expected {numbers}')\n",
    "        if len(numbers) == 0:\n",
    "            print(f'no numbers found')\n",
    "            index = 0\n",
    "        else:\n",
    "            index = int(numbers[0]) - 1\n",
    "            if index > 4:\n",
    "                print(f'Index is out of bounds. Something went wrong with the API Answer. Defaulting to 0')\n",
    "                index = 0\n",
    "\n",
    "        print(f'{i}) answer: {index}')\n",
    "        print('--------------------------------')\n",
    "        predictions.append(generations[index])\n",
    "\n",
    "    return predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k=5\n",
    "pipeline = Text2TextGenerationPipeline(model=model, tokenizer=tokenizer, max_length=config.generation_max_length, num_beams=k, num_return_sequences=k, device=0)\n",
    "\n",
    "samples = list(val_df.raw_sentence)\n",
    "resolutions = list(val_df.full_resolution)\n",
    "outputs = pipeline(samples)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = generate_best_fit(samples, resolutions, outputs)\n",
    "\n",
    "errors = error_analysis(predictions, resolutions, samples)\n",
    "scores = get_scores(errors, \"eval\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
